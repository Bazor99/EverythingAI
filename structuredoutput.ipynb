{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNP+hPYSZMvaqj17SJqVpPP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bazor99/EverythingAI/blob/main/structuredoutput.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STTg9sJUuyh3",
        "outputId": "4ea02ce8-1580-4f6f-c07b-3154b649b796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.4.3-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.11/dist-packages (0.3.42)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.59)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.25-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt>=0.1.8 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.69-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.11.4)\n",
            "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith) (3.10.18)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (24.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.16.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (4.13.2)\n",
            "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph)\n",
            "  Downloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith) (2.4.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith) (1.3.1)\n",
            "Downloading langgraph-0.4.3-py3-none-any.whl (151 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.2/151.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.25-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.1.8-py3-none-any.whl (25 kB)\n",
            "Downloading langgraph_sdk-0.1.69-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "Successfully installed langgraph-0.4.3 langgraph-checkpoint-2.0.25 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.69 ormsgpack-1.9.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langgraph langsmith"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU \"langchain[openai]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4AzLmG5u2ur",
        "outputId": "0411bdb1-e895-4da6-f896-ca7f4016e4f7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m61.4/62.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ml4hkkPw0nIQ",
        "outputId": "915fc207-a6e2-4c48-cf37-43275c6cc048"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter API key for OpenAI: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "# Pydantic\n",
        "class Joke(BaseModel):\n",
        "    \"\"\"Joke to tell user.\"\"\"\n",
        "\n",
        "    setup: str = Field(description=\"The setup of the joke\")\n",
        "    punchline: str = Field(description=\"The punchline to the joke\")\n",
        "    rating: Optional[int] = Field(\n",
        "        default=None, description=\"How funny the joke is, from 1 to 10\"\n",
        "    )\n",
        "\n",
        "\n",
        "structured_llm = llm.with_structured_output(Joke)\n",
        "\n",
        "structured_llm.invoke(\"Tell me a joke about cats\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba7oLXhq1M6_",
        "outputId": "b30970dd-ed83-4d52-ed2f-689107d6665c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Joke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=8)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "from typing_extensions import Annotated, TypedDict\n",
        "\n",
        "\n",
        "# TypedDict\n",
        "class Joke(TypedDict):\n",
        "    \"\"\"Joke to tell user.\"\"\"\n",
        "\n",
        "    setup: Annotated[str, ..., \"The setup of the joke\"]\n",
        "\n",
        "    # Alternatively, we could have specified setup as:\n",
        "\n",
        "    # setup: str                    # no default, no description\n",
        "    # setup: Annotated[str, ...]    # no default, no description\n",
        "    # setup: Annotated[str, \"foo\"]  # default, no description\n",
        "\n",
        "    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n",
        "    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n",
        "\n",
        "\n",
        "structured_llm = llm.with_structured_output(Joke)\n",
        "\n",
        "structured_llm.invoke(\"Tell me a joke about cats\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8lj-PNo3BfX",
        "outputId": "36c71fb1-87d7-42dd-ce1f-e0082c1ccbe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'setup': 'Why did the cat sit on the computer?',\n",
              " 'punchline': 'Because it wanted to keep an eye on the mouse!',\n",
              " 'rating': 7}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_schema = {\n",
        "    \"title\": \"joke\",\n",
        "    \"description\": \"Joke to tell user.\",\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"setup\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The setup of the joke\",\n",
        "        },\n",
        "        \"punchline\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The punchline to the joke\",\n",
        "        },\n",
        "        \"rating\": {\n",
        "            \"type\": \"integer\",\n",
        "            \"description\": \"How funny the joke is, from 1 to 10\",\n",
        "            \"default\": None,\n",
        "        },\n",
        "    },\n",
        "    \"required\": [\"setup\", \"punchline\"],\n",
        "}\n",
        "structured_llm = llm.with_structured_output(json_schema)\n",
        "\n",
        "structured_llm.invoke(\"Tell me a joke about cats\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zSb2ooG4b88",
        "outputId": "8954fd62-d3ac-4514-8b1a-cfcaeb16d2b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'setup': 'Why was the cat sitting on the computer?',\n",
              " 'punchline': 'Because it wanted to keep an eye on the mouse!',\n",
              " 'rating': 7}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Union\n",
        "\n",
        "\n",
        "class Joke(BaseModel):\n",
        "    \"\"\"Joke to tell user.\"\"\"\n",
        "\n",
        "    setup: str = Field(description=\"The setup of the joke\")\n",
        "    punchline: str = Field(description=\"The punchline to the joke\")\n",
        "    rating: Optional[int] = Field(\n",
        "        default=None, description=\"How funny the joke is, from 1 to 10\"\n",
        "    )\n",
        "\n",
        "\n",
        "class ConversationalResponse(BaseModel):\n",
        "    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n",
        "\n",
        "    response: str = Field(description=\"A conversational response to the user's query\")\n",
        "\n",
        "\n",
        "class FinalResponse(BaseModel):\n",
        "    final_output: Union[Joke, ConversationalResponse]\n",
        "\n",
        "\n",
        "structured_llm = llm.with_structured_output(FinalResponse)\n",
        "\n",
        "structured_llm.invoke(\"Tell me a joe about kthe ugly vulture\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkxJ9dFh4lHm",
        "outputId": "20fd32fe-9af6-4518-a3c6-2c0ac98f39c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FinalResponse(final_output=Joke(setup='Why did the ugly vulture get kicked out of the party?', punchline='Because he was too much of a carrion!', rating=7))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "structured_llm.invoke(\"How are you today?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Juk-leF3Bb-",
        "outputId": "f01f05f5-2c32-450f-aee7-5a26f0614253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FinalResponse(final_output=ConversationalResponse(response=\"I'm just a computer program, but I'm here and ready to assist you! How about you? How's your day going?\"))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import Annotated, TypedDict\n",
        "\n",
        "\n",
        "# TypedDict\n",
        "class Joke(TypedDict):\n",
        "    \"\"\"Joke to tell user.\"\"\"\n",
        "\n",
        "    setup: Annotated[str, ..., \"The setup of the joke\"]\n",
        "    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n",
        "    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n",
        "\n",
        "\n",
        "structured_llm = llm.with_structured_output(Joke)\n",
        "\n",
        "for chunk in structured_llm.stream(\"Tell me a joke about cats\"):\n",
        "    print(chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bG54DEYG7pvy",
        "outputId": "b79197cf-a93c-41c6-e3c3-eb0d1e84a940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{}\n",
            "{'setup': ''}\n",
            "{'setup': 'Why'}\n",
            "{'setup': 'Why was'}\n",
            "{'setup': 'Why was the'}\n",
            "{'setup': 'Why was the cat'}\n",
            "{'setup': 'Why was the cat sitting'}\n",
            "{'setup': 'Why was the cat sitting on'}\n",
            "{'setup': 'Why was the cat sitting on the'}\n",
            "{'setup': 'Why was the cat sitting on the computer'}\n",
            "{'setup': 'Why was the cat sitting on the computer?'}\n",
            "{'setup': 'Why was the cat sitting on the computer?', 'punchline': ''}\n",
            "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because'}\n",
            "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it'}\n",
            "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted'}\n",
            "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to'}\n",
            "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep'}\n",
            "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an'}\n",
            "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye'}\n",
            "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on'}\n",
            "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the'}\n",
            "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse'}\n",
            "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!'}\n",
            "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!', 'rating': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Few Shot Prompting"
      ],
      "metadata": {
        "id": "zCwAX8ZX8T32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\\n",
        "Return a joke which has the setup (the response to \"Who's there?\") and the final punchline (the response to \"<setup> who?\").\n",
        "\n",
        "Here are some examples of jokes:\n",
        "\n",
        "example_user: Tell me a joke about planes\n",
        "example_assistant: {{\"setup\": \"Why don't planes ever get tired?\", \"punchline\": \"Because they have rest wings!\", \"rating\": 2}}\n",
        "\n",
        "example_user: Tell me another joke about planes\n",
        "example_assistant: {{\"setup\": \"Cargo\", \"punchline\": \"Cargo 'vroom vroom', but planes go 'zoom zoom'!\", \"rating\": 10}}\n",
        "\n",
        "example_user: Now about caterpillars\n",
        "example_assistant: {{\"setup\": \"Caterpillar\", \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\", \"rating\": 5}}\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{input}\")])\n",
        "\n",
        "few_shot_structured_llm = prompt | structured_llm\n",
        "few_shot_structured_llm.invoke(\"what's something funny about woodpeckers\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4X22tCo37oZ5",
        "outputId": "f60ced9a-6e49-4bab-a1d9-babf497087c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'setup': 'Woodpecker',\n",
              " 'punchline': \"Woodpecker knock, knockin' on wood! What's the matter, can't find a door?\",\n",
              " 'rating': 7}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage\n",
        "\n",
        "examples = [\n",
        "    HumanMessage(\"Tell me a joke about planes\", name=\"example_user\"),\n",
        "    AIMessage(\n",
        "        \"\",\n",
        "        name=\"example_assistant\",\n",
        "        tool_calls=[\n",
        "            {\n",
        "                \"name\": \"joke\",\n",
        "                \"args\": {\n",
        "                    \"setup\": \"Why don't planes ever get tired?\",\n",
        "                    \"punchline\": \"Because they have rest wings!\",\n",
        "                    \"rating\": 2,\n",
        "                },\n",
        "                \"id\": \"1\",\n",
        "            }\n",
        "        ],\n",
        "    ),\n",
        "    # Most tool-calling models expect a ToolMessage(s) to follow an AIMessage with tool calls.\n",
        "    ToolMessage(\"\", tool_call_id=\"1\"),\n",
        "    # Some models also expect an AIMessage to follow any ToolMessages,\n",
        "    # so you may need to add an AIMessage here.\n",
        "    HumanMessage(\"Tell me another joke about planes\", name=\"example_user\"),\n",
        "    AIMessage(\n",
        "        \"\",\n",
        "        name=\"example_assistant\",\n",
        "        tool_calls=[\n",
        "            {\n",
        "                \"name\": \"joke\",\n",
        "                \"args\": {\n",
        "                    \"setup\": \"Cargo\",\n",
        "                    \"punchline\": \"Cargo 'vroom vroom', but planes go 'zoom zoom'!\",\n",
        "                    \"rating\": 10,\n",
        "                },\n",
        "                \"id\": \"2\",\n",
        "            }\n",
        "        ],\n",
        "    ),\n",
        "    ToolMessage(\"\", tool_call_id=\"2\"),\n",
        "    HumanMessage(\"Now about caterpillars\", name=\"example_user\"),\n",
        "    AIMessage(\n",
        "        \"\",\n",
        "        tool_calls=[\n",
        "            {\n",
        "                \"name\": \"joke\",\n",
        "                \"args\": {\n",
        "                    \"setup\": \"Caterpillar\",\n",
        "                    \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\",\n",
        "                    \"rating\": 5,\n",
        "                },\n",
        "                \"id\": \"3\",\n",
        "            }\n",
        "        ],\n",
        "    ),\n",
        "    ToolMessage(\"\", tool_call_id=\"3\"),\n",
        "]\n",
        "system = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\\n",
        "Return a joke which has the setup (the response to \"Who's there?\") \\\n",
        "and the final punchline (the response to \"<setup> who?\").\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", system), (\"placeholder\", \"{examples}\"), (\"human\", \"{input}\")]\n",
        ")\n",
        "few_shot_structured_llm = prompt | structured_llm\n",
        "few_shot_structured_llm.invoke({\"input\": \"crocodiles\", \"examples\": examples})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjG3Y_in7nsB",
        "outputId": "169c2e3c-1e30-4826-968e-ffb2f3989503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'setup': 'Crocodile',\n",
              " 'punchline': \"Crocodile you believe it? I'm just a little in-de-Nile!\",\n",
              " 'rating': 7}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Advanced method for structing output"
      ],
      "metadata": {
        "id": "F2sqi5zk_5wJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "structured_llm = llm.with_structured_output(None, method=\"json_mode\")\n",
        "\n",
        "structured_llm.invoke(\n",
        "    \"Tell me a joke about cats, respond in JSON with 'rating', `setup` and `punchline` keys\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1a8iLkE-3Vl",
        "outputId": "454972a7-93ea-4aed-a276-88c148c06761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rating': 5,\n",
              " 'setup': 'Why did the cat sit on the computer?',\n",
              " 'punchline': 'Because it wanted to keep an eye on the mouse!'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Advanced raw *output*"
      ],
      "metadata": {
        "id": "kLRjwoQuBnAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "structured_llm = llm.with_structured_output(Joke, include_raw=True)\n",
        "\n",
        "structured_llm.invoke(\"Tell me a joke about cats\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL3gX-V5BWRP",
        "outputId": "f96c4dee-b179-4f9d-e668-45c8d287b1e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'raw': AIMessage(content='{\"setup\":\"Why was the cat sitting on the computer?\",\"punchline\":\"Because it wanted to keep an eye on the mouse!\",\"rating\":7}', additional_kwargs={'parsed': None, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 96, 'total_tokens': 130, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_129a36352a', 'id': 'chatcmpl-BWkqXwgnMBvMoR0yEo9FxzwNPRbaO', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c06d2b2e-b94d-410d-93e0-e5dd3bb40222-0', usage_metadata={'input_tokens': 96, 'output_tokens': 34, 'total_tokens': 130, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " 'parsed': {'setup': 'Why was the cat sitting on the computer?',\n",
              "  'punchline': 'Because it wanted to keep an eye on the mouse!',\n",
              "  'rating': 7},\n",
              " 'parsing_error': None}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pydantic output parser"
      ],
      "metadata": {
        "id": "OMsFbtGPAQXY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To parse the output of a chat model prompted to match the given Pydantic schema. Note that we are adding format_instructions directly to the prompt from a method on the parser"
      ],
      "metadata": {
        "id": "Ow1bRzu5Ak3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class Person(BaseModel):\n",
        "    \"\"\"Information about a person.\"\"\"\n",
        "\n",
        "    name: str = Field(..., description=\"The name of the person\")\n",
        "    height_in_meters: float = Field(\n",
        "        ..., description=\"The height of the person expressed in meters.\"\n",
        "    )\n",
        "\n",
        "\n",
        "class People(BaseModel):\n",
        "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
        "\n",
        "    people: List[Person]\n",
        "\n",
        "\n",
        "# Set up a parser\n",
        "parser = PydanticOutputParser(pydantic_object=People)\n",
        "\n",
        "# Prompt\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\",\n",
        "        ),\n",
        "        (\"human\", \"{query}\"),\n",
        "    ]\n",
        ").partial(format_instructions=parser.get_format_instructions())"
      ],
      "metadata": {
        "id": "SsDHj71fCHHe"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Anna is 23 years old and she is 6 feet tall\"\n",
        "\n",
        "print(prompt.invoke({\"query\": query}).to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooJMPZyfB-kY",
        "outputId": "26e7e012-804b-4b03-b359-e8587308bd96"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: Answer the user query. Wrap the output in `json` tags\n",
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"$defs\": {\"Person\": {\"description\": \"Information about a person.\", \"properties\": {\"name\": {\"description\": \"The name of the person\", \"title\": \"Name\", \"type\": \"string\"}, \"height_in_meters\": {\"description\": \"The height of the person expressed in meters.\", \"title\": \"Height In Meters\", \"type\": \"number\"}}, \"required\": [\"name\", \"height_in_meters\"], \"title\": \"Person\", \"type\": \"object\"}}, \"description\": \"Identifying information about all people in a text.\", \"properties\": {\"people\": {\"items\": {\"$ref\": \"#/$defs/Person\"}, \"title\": \"People\", \"type\": \"array\"}}, \"required\": [\"people\"]}\n",
            "```\n",
            "Human: Anna is 23 years old and she is 6 feet tall\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm | parser\n",
        "\n",
        "chain.invoke({\"query\": query})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V68nTS9-CXdD",
        "outputId": "7005fbb9-4737-4402-b8e2-ececf3e0c6a5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "People(people=[Person(name='Anna', height_in_meters=1.8288)])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Parsing"
      ],
      "metadata": {
        "id": "O33dALdODNff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from typing import List\n",
        "\n",
        "from langchain_core.messages import AIMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class Person(BaseModel):\n",
        "    \"\"\"Information about a person.\"\"\"\n",
        "\n",
        "    name: str = Field(..., description=\"The name of the person\")\n",
        "    height_in_meters: float = Field(\n",
        "        ..., description=\"The height of the person expressed in meters.\"\n",
        "    )\n",
        "\n",
        "\n",
        "class People(BaseModel):\n",
        "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
        "\n",
        "    people: List[Person]\n",
        "\n",
        "\n",
        "# Prompt\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Answer the user query. Output your answer as JSON that  \"\n",
        "            \"matches the given schema: \\`\\`\\`json\\n{schema}\\n\\`\\`\\`. \"\n",
        "            \"Make sure to wrap the answer in \\`\\`\\`json and \\`\\`\\` tags\",\n",
        "        ),\n",
        "        (\"human\", \"{query}\"),\n",
        "    ]\n",
        ").partial(schema=People.schema())\n",
        "\n",
        "\n",
        "# Custom parser\n",
        "def extract_json(message: AIMessage) -> List[dict]:\n",
        "    \"\"\"Extracts JSON content from a string where JSON is embedded between \\`\\`\\`json and \\`\\`\\` tags.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The text containing the JSON content.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of extracted JSON strings.\n",
        "    \"\"\"\n",
        "    text = message.content\n",
        "    # Define the regular expression pattern to match JSON blocks\n",
        "    pattern = r\"\\`\\`\\`json(.*?)\\`\\`\\`\"\n",
        "\n",
        "    # Find all non-overlapping matches of the pattern in the string\n",
        "    matches = re.findall(pattern, text, re.DOTALL)\n",
        "\n",
        "    # Return the list of matched JSON strings, stripping any leading or trailing whitespace\n",
        "    try:\n",
        "        return [json.loads(match.strip()) for match in matches]\n",
        "    except Exception:\n",
        "        raise ValueError(f\"Failed to parse: {message}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t751zpcGDNBH",
        "outputId": "cf9b5277-c91d-44cc-e6e2-99fc214dc093"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-39d9d3865b28>:36: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  ).partial(schema=People.schema())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Anna is 23 years old and she is 6 feet tall\"\n",
        "\n",
        "print(prompt.format_prompt(query=query).to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NReozOSPDM9Q",
        "outputId": "8094ee59-39ed-41af-b6d8-dce0a69cabde"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: Answer the user query. Output your answer as JSON that  matches the given schema: \\`\\`\\`json\n",
            "{'$defs': {'Person': {'description': 'Information about a person.', 'properties': {'name': {'description': 'The name of the person', 'title': 'Name', 'type': 'string'}, 'height_in_meters': {'description': 'The height of the person expressed in meters.', 'title': 'Height In Meters', 'type': 'number'}}, 'required': ['name', 'height_in_meters'], 'title': 'Person', 'type': 'object'}}, 'description': 'Identifying information about all people in a text.', 'properties': {'people': {'items': {'$ref': '#/$defs/Person'}, 'title': 'People', 'type': 'array'}}, 'required': ['people'], 'title': 'People', 'type': 'object'}\n",
            "\\`\\`\\`. Make sure to wrap the answer in \\`\\`\\`json and \\`\\`\\` tags\n",
            "Human: Anna is 23 years old and she is 6 feet tall\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm | extract_json\n",
        "\n",
        "chain.invoke({\"query\": query})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05l2hB7TDM1P",
        "outputId": "5ecbb38b-a80f-49c4-a61f-7ef29e52b8c0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'people': [{'name': 'Anna', 'height_in_meters': 1.829}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ufRK7owTDtly"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}